Training Data: https://commonvoice.mozilla.org/en/datasets**
Pytorch Implementation: https://github.com/elyxlz/voxtral/tree/master
Audax: Audio Extraction Jax https://github.com/SarthakYadav/audax
* [Pytorch Implementation](https://docs.pytorch.org/audio/stable/index.html)
https://github.com/yixiaoer/mistral-v0.2-jax

https://github.com/sanchit-gandhi/whisper-jax

[Porting a Pytorch model to Jax](https://docs.jaxstack.ai/en/latest/JAX_porting_PyTorch_model.html)
[Model Implementation Transformers](https://github.com/huggingface/transformers/blob/main/src/transformers/models/voxtral/modular_voxtral.py)
https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_flax_mistral.py
https://github.com/sanchit-gandhi/whisper-jax/blob/main/whisper_jax/modeling_flax_whisper.py

**Key Components ([based on the paper](https://arxiv.org/pdf/2507.13264))**

For now we can just leverage a few existing pieces
* WhisperProcessor as the audio Processor 
* AdapterLayer is the main thing I need to do, hoping can copy the pytorch implementation
* MistralForCausalLM as the Language Decoder

Main things need to do
* Adapter layer to take whisper output into MistralForCausalLM input
* Convert the adapter layer weights for this
* Do I need to convert the Mistral model to jax as well?







Model Display Output:

MistralForCausalLM(# Attributes:config=# <transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>MistralConfig { "_name_or_path": "mistralai/Voxtral-Mini-3B-2507", "architectures": [ "VoxtralForConditionalGeneration" ], "audio_config": { "activation_dropout": 0.0, "activation_function": "gelu", "attention_dropout": 0.0, "dropout": 0.0, "head_dim": 64, "hidden_size": 1280, "initializer_range": 0.02, "intermediate_size": 5120, "layerdrop": 0.0, "max_source_positions": 1500, "model_type": "voxtral_encoder", "num_attention_heads": 20, "num_hidden_layers": 32, "num_key_value_heads": 20, "num_mel_bins": 128, "scale_embedding": false, "vocab_size": 51866 }, "audio_token_id": 24, "bos_token_id": 1, "eos_token_id": 2, "hidden_act": "silu", "hidden_size": 3072, "initializer_range": 0.02, "intermediate_size": 14336, "max_position_embeddings": 131072, "model_type": "mistral", "num_attention_heads": 32, "num_hidden_layers": 32, "num_key_value_heads": 8, "projector_hidden_act": "gelu", "rms_norm_eps": 1e-06, "rope_theta": 10000.0, "sliding_window": 4096, "text_config": { "attention_bias": false, "attention_dropout": 0.0, "head_dim": 128, "hidden_act": "silu", "hidden_size": 3072, "initializer_range": 0.02, "intermediate_size": 8192, "max_position_embeddings": 131072, "mlp_bias": false, "model_type": "llama", "num_attention_heads": 32, "num_hidden_layers": 30, "num_key_value_heads": 8, "pretraining_tp": 1, "rms_norm_eps": 1e-05, "rope_scaling": null, "rope_theta": 100000000.0, "sliding_window": null, "use_cache": true, "vocab_size": 131072 }, "tie_word_embeddings": false, "torch_dtype": "bfloat16", "transformers_version": "4.34.1", "use_cache": true, "vocab_size": 131072 } , # Repeated python obj at 0x7fcfa9fcb810name_or_path='mistralai/Voxtral-Mini-3B-2507',warnings_issued={},generation_config=<transformers.generation.configuration_utils.GenerationConfig object at 0x7fcfaa129090>,vocab_size=131072,is_loaded_in_4bit=False,is_loaded_in_8bit=False,training=False,# Child modules:model=MistralModel(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, name_or_path='mistralai/Voxtral-Mini-3B-2507', warnings_issued={}, generation_config=None, padding_idx=None, vocab_size=131072, gradient_checkpointing=False, training=False, # Child modules:embed_tokens=Embedding(num_embeddings=131072, embedding_dim=3072, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, training=False, # Parameters:weight=<torch.nn.Parameter float32(131072, 3072)>,),layers=ModuleList(training=False,# Child modules:(0): MistralDecoderLayer(hidden_size=3072, training=False, # Child modules:self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, # Child modules:q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ),k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ),v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ),o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ),rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ),),mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, # Child modules:gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ),up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ),down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ),act_fn=SiLUActivation(training=False, ),),input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, # Parameters:weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>,),post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, # Parameters:weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>,),),(1): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(2): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(3): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(4): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(5): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(6): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(7): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(8): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(9): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(10): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(11): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(12): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(13): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(14): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(15): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(16): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(17): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(18): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(19): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(20): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(21): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(22): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(23): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(24): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(25): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(26): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(27): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(28): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(29): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(30): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),(31): MistralDecoderLayer(hidden_size=3072, training=False, self_attn=MistralAttention(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, num_heads=32, head_dim=96, num_key_value_heads=8, num_key_value_groups=4, max_position_embeddings=131072, rope_theta=10000.0, training=False, q_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), k_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), v_proj=Linear(in_features=3072, out_features=768, training=False, weight=<torch.nn.Parameter float32(768, 3072)>, ), o_proj=Linear(in_features=3072, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 3072)>, ), rotary_emb=MistralRotaryEmbedding(dim=96, max_position_embeddings=131072, base=10000.0, max_seq_len_cached=131072, training=False, inv_freq=<torch.Tensor float32(48,) ≈0.12 ±0.051 [≥0.00012, ≤1.0] nonzero:48>, cos_cached=<torch.Tensor float32(1, 1, 131072, 96)>, sin_cached=<torch.Tensor float32(1, 1, 131072, 96)>, ), ), mlp=MistralMLP(config=<transformers.models.mistral.configuration_mistral.MistralConfig object at 0x7fcfa9fcb810>, hidden_size=3072, intermediate_size=14336, training=False, gate_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), up_proj=Linear(in_features=3072, out_features=14336, training=False, weight=<torch.nn.Parameter float32(14336, 3072)>, ), down_proj=Linear(in_features=14336, out_features=3072, training=False, weight=<torch.nn.Parameter float32(3072, 14336)>, ), act_fn=SiLUActivation(training=False, ), ), input_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), post_attention_layernorm=MistralRMSNorm(variance_epsilon=1e-06, training=False, weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>, ), ),),norm=MistralRMSNorm(variance_epsilon=1e-06, training=False, # Parameters:weight=<torch.nn.Parameter float32(3072,) ≈1.0 ±0.0 [≥1.0, ≤1.0] nonzero:3_072>,),),lm_head=Linear(in_features=3072, out_features=131072, training=False, # Parameters:weight=<torch.nn.Parameter float32(131072, 3072)>,),)