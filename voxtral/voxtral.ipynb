{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/obbwins/whisper-jax.git@fix-jax-compatibility\n",
    "!pip install librosa soundfile\n",
    "!pip install accelerate==0.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from datasets import load_dataset\n",
    "from jax import device_get, jit\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "from whisper_jax import FlaxWhisperForConditionalGeneration\n",
    "\n",
    "# load the processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "model, params = FlaxWhisperForConditionalGeneration.from_pretrained(\n",
    "    \"openai/whisper-base\", _do_init=False,\n",
    ")\n",
    "\n",
    "def generate_fn(input_features, params):\n",
    "    pred_ids = model.generate(\n",
    "        input_features, \n",
    "        task=\"transcribe\", \n",
    "        return_timestamps=False, \n",
    "        max_length=model.config.max_length, \n",
    "        params=params,\n",
    "    )\n",
    "    return pred_ids.sequences\n",
    "\n",
    "# jit the generate function for speed\n",
    "p_generate = jit(generate_fn)\n",
    "\n",
    "# load a dummy sample from the LibriSpeech dataset\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "\n",
    "# pre-process: convert the audio array to log-mel input features\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"np\").input_features\n",
    "\n",
    "# Always pass params when calling model.encode!\n",
    "output = model.encode(input_features=input_features, params=params)\n",
    "last_hidden_state = output.last_hidden_state\n",
    "\n",
    "print(last_hidden_state)\n",
    "print(last_hidden_state.shape)\n",
    "\n",
    "# run the forward pass (JIT compiled the first time it is called)\n",
    "pred_ids = p_generate(input_features, params)\n",
    "\n",
    "# post-process: convert tokens ids to text string\n",
    "transcription = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "\n",
    "class AudioAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Audio adapter module that downsamples audio embeddings using a 1D\n",
    "    convolutional layer for a 4x temporal downsampling.\n",
    "    \"\"\"\n",
    "    embedding_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the adapter.\n",
    "        \n",
    "        Args:\n",
    "            x (jnp.ndarray): The input tensor from the audio encoder.\n",
    "                             Expected shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        Returns:\n",
    "            jnp.ndarray: The downsampled output tensor.\n",
    "                         Expected shape: (batch_size, new_sequence_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Flax Conv layer expects channel-last format (NLC), which the input\n",
    "        # tensor (1, 1500, 512) already is, so no permutation is needed.\n",
    "        downsampler = nn.Conv(\n",
    "            features=self.embedding_dim,\n",
    "            kernel_size=(4,),\n",
    "            strides=(4,)\n",
    "        )\n",
    "        x = downsampler(x)\n",
    "        return x\n",
    "\n",
    "# Example Usage:\n",
    "# Define the input tensor based on your specifications\n",
    "input_tensor = last_hidden_state\n",
    "\n",
    "# Generate a random key for parameter initialization\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Instantiate the adapter with the specified embedding dimension\n",
    "embedding_dim = input_tensor.shape[2]\n",
    "adapter = AudioAdapter(embedding_dim=embedding_dim)\n",
    "\n",
    "# Initialize the model parameters\n",
    "params = adapter.init(key, input_tensor)['params']\n",
    "\n",
    "# Perform a forward pass\n",
    "output_tensor = adapter.apply({'params': params}, input_tensor)\n",
    "\n",
    "print(f\"Original input tensor shape: {input_tensor.shape}\")\n",
    "print(f\"Downsampled output tensor shape: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, w, MistralForCausalLM\n",
    "\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "\n",
    "# Step 1: Initialize the processor and model\n",
    "# model_name = \"mistralai/Mistral-Small-24B-Base-2501\"\n",
    "model_name = \"mistralai/Voxtral-Mini-3B-2507\"\n",
    "# model_name = \"mistralai/Magistral-Small-2506\"\n",
    "# model_name = \"ministral/Ministral-3b-instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, token = token)\n",
    "model = MistralForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "from flax import nnx\n",
    "nnx.display(model)\n",
    "# # Step 2: Define your input embeddings\n",
    "# # The user-provided audio embeddings of shape (1, 375, 512).\n",
    "# # The processor expects a list of embeddings.\n",
    "# audio_embeddings = output_tensor\n",
    "\n",
    "# # The user-provided text embeddings for the question.\n",
    "# text_question = \"What is the primary topic of the audio?\"\n",
    "\n",
    "# # Step 3: Use the processor to create a single input format\n",
    "# # The processor combines the audio and text inputs into a format the model can use.\n",
    "# inputs = processor(\n",
    "#     audios=audio_embeddings,\n",
    "#     text=text_question,\n",
    "#     return_tensors=\"jax\"\n",
    "# )\n",
    "\n",
    "# # Step 4: Generate a text response using the JAX model\n",
    "# # The model will take the combined inputs and generate a text output.\n",
    "# # You can customize generation parameters like max_new_tokens.\n",
    "# outputs = model.generate(\n",
    "#     input_ids=inputs.input_ids,\n",
    "#     attention_mask=inputs.attention_mask,\n",
    "#     audio_embeddings=inputs.audio_embeddings,\n",
    "#     max_new_tokens=50\n",
    "# )\n",
    "\n",
    "# # Step 5: Decode the output to get the final text response\n",
    "# response = processor.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\n",
    "\n",
    "# print(\"Generated Response:\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Choose the model size you want to use\n",
    "model_name = \"mistralai/Voxtral-Mini-3B-2507\" # Or use \"mistralai/Voxtral-Small-24B-2507\"\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, FlaxAutoModelForCausalLM, MistralForCausalLM\n",
    "\n",
    "\n",
    "# Step 1: Initialize the processor and model\n",
    "# model_name = \"mistralai/Mistral-Small-24B-Base-2501\"\n",
    "model_name = \"mistralai/Voxtral-Mini-3B-2507\"\n",
    "# model_name = \"mistralai/Magistral-Small-2506\"\n",
    "# model_name = \"ministral/Ministral-3b-instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, token = token)\n",
    "model = MistralForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "from flax import nnx\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
